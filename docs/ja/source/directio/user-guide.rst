=========================
Direct I/O ユーザーガイド
=========================

この文書では、Direct I/Oの利用方法について紹介します。
Direct I/Oの導入方法については :doc:`../administration/deployment-with-directio` を参照してください。


データソース
============
Direct I/Oを利用したバッチアプリケーションでは、ジョブフローの入出力データをHadoopから直接読み書きします。
このとき、読み書きする対象のことを「データソース」として抽象化しています。

それぞれのデータソースはいずれかの「論理パス」上に配置します。
このパスは唯一のルートを持つツリー状の構造で、ルートを含む任意のノードに対してデータソースを配置できます。
これらの配置設定はAsakusa Framework側の設定ファイル上で行います。

DSLではデータソースそのものを指定せずに、入出力を行う先の論理パスのみを指定します。
ジョブフローの実行時には論理パスからデータソースを引き当て、そのデータソースに対する入出力が行われます。


入力のアーキテクチャ
====================
Direct I/Oの入力には次のような特徴があります。

* 大きなデータを分割して分散して読みだす

以降では上記の特徴に関するアーキテクチャを紹介します。

入力データの分割
----------------
Direct I/Oでデータを読み出す際、以下の条件をすべて満たした場合にデータを小さな断片に分割し、断片ごとに分散して読み出します。

* データソースが分割をサポートしている
* データの形式が分割をサポートしている
* データが十分に大きい

基本的にこの機能はHadoopの「入力スプリット」の仕組みを利用して実現しています。
ただし、データソースごとに細かく分割の単位を指定できるなどの相違があります。

..  warning::
    バージョン |version| では、同一の入力を複数回読む場合があります。
    これは将来のバージョンで改善される予定です。

分散キャッシュと連携する場合の動作
----------------------------------
Asakusa Frameworkでは入力データをHadoopの分散キャッシュに乗せて、処理を高速化するような最適化が行われる場合があります。
そのような場合、入力データはHadoopのファイルシステム上に一度だけコピーしたのち、コピーした内容をスレーブノードに配布しています。


出力のアーキテクチャ
====================
Direct I/Oの出力には次のような特徴があります。

* 出力ファイルを内容によって分割したりソートしたりできる
* Direct I/O全体の出力を簡易的なトランザクションで行う

以降では上記の特徴に関するアーキテクチャを紹介します。

出力ファイルの分割と内容のソート
--------------------------------
Direct I/Oでファイルを出力する際、出力内容を元にファイルの分割と内容のソートを行っています。
これは現在、MapReduceの仕組みの上で以下のように行っています。

* Mapperで出力内容のファイル名、順序計算のためのプロパティからシャッフル用のキーを生成する
* シャッフル時にファイル名でグルーピングし、さらに順序計算のプロパティをもとにソートする (セカンダリソート)
* Reduceではファイル名ごとにグループができるため、グループごとにファイルを一つ出力する

このため、出力ファイルが1つである場合や、大きさに極端な偏りが見られる場合には全体の性能が低下します。

Direct I/Oでは出力ファイル名の生成パターンを指定して出力ファイルを分割する、もしくは
出力ファイル名の生成パターンを細かく指定せず、分散環境に都合のよい任意の値を使用する [#]_ ことで
Reduceを行わず高速に動作するなどといった設定することが可能です。

..  [#] この機能を使う場合はソートが使えなくなるなど、出力ファイルの指定にはいくつかのトレードオフが存在します。詳細は後述の `出力ファイル名のパターン`_ を参照してください。

簡易的な出力のトランザクション
------------------------------
Direct I/Oで複数のデータソースに対して出力を行う際、内部では簡易的なトランザクション処理を行っています。
これにより、いずれかのデータソースに出力する際にエラーとなってしまった場合でも、最終的な出力先のデータを全く破壊しないか、
または整合性のとれた出力結果を得られるようになっています。

Direct I/Oのトランザクション処理は主に以下の流れで行います。

1. システムディレクトリ [#]_ に「トランザクション情報ファイル」を作成する
2. それぞれのMapReduceのタスク試行で、出力をデータソースの「試行領域 (attempt area)」に書き出す
3. タスク試行が成功した場合、それぞれの試行領域のデータをデータソースの「ステージング領域 (staging area)」に移動する

  * 失敗した場合は試行領域のデータをクリアする

4. すべてのタスクが成功した場合、システムディレクトリに「コミットマークファイル」をアトミックに作成する

  * いずれかが失敗した場合はコミットマークファイルを作成しない

5. コミットマークファイルが存在する場合、それぞれのステージング領域の内容を最終的な出力先に移動し、コミットマークファイルを削除する

  * コミットマークファイルが存在しない場合は、ステージング領域の内容をクリアする

6. トランザクション情報ファイルを削除する

なお、試行領域はタスク試行ごとに、ステージング領域はデータソースごとにそれぞれ作成されます。

..  attention::
    標準的なデータソースでは、すでに移動先にデータが存在する場合に上書きします。

上記の仕組み上、Direct I/Oによる出力には次のような制約があります。

* 試行領域 > ステージング領域 > 最終的な出力先 とデータを移動させるため、データの移動に時間がかかるデータソースでは速度が出ない [#]_
* コミットマークファイル作成から削除までの間、データソースは一時的に整合性が失われる
* コミットマークファイル作成から削除までの間に処理が失敗した場合、修復が行われるまで整合性が失われる [#]_
* コミットマークファイル自体が障害によって失われた場合、データソースの整合性が失われる
* 同一の出力先に対して複数のジョブフローから出力を行った場合、結果が不安定になる (競合に対するロック等は行わない)

..  [#] 設定方法については `システムディレクトリの設定`_ を参照してください。
..  [#] 例えばHadoopファイルシステムを経由して Amazon Simple Storage Service ( `Amazon S3`_ )を利用する場合、データの移動に時間がかかるようです。後述の `Amazon S3での設定例`_ も参考にしてください。
..  [#] 修復手順は `トランザクションのメンテナンス`_ を参照してください。

..  warning::
    バージョン |version| ではトランザクションの修復を自動的には行いません。
    `トランザクションのメンテナンス`_ を参考に、手動で修復を行ってください。

..  _`Amazon S3`: http://aws.amazon.com/s3/

データソースの設定
==================
Direct I/Oの機構を利用するには、入出力の仲介を行う「データソース」の設定が必要です。
主に以下のような設定を行います。

* データソースの実装
* データソースを配置する論理パス
* データソースが実際に利用するファイルシステム上のパス

これらの設定は、 ``$ASAKUSA_HOME`` で指定したディレクトリ以下の ``core/conf/asakusa-resources.xml`` (以下、設定ファイル)内に、以下の形式でそれぞれ記述していきます。

..  code-block:: xml

    <property>
        <name>プロパティ名</name>
        <value>値</value>
    </property>

..  note::
    このファイルはAsakusa FrameworkがHadoopのジョブを実行する際に利用する共通の設定ファイル [#]_ です。
    Hadoop本体の ``core-site.xml`` 等と同様の形式 [#]_ ですが、 ``${...}`` 形式での
    システムプロパティの展開をサポートしていません。

..  [#] :doc:`実行時プラグイン <../administration/deployment-runtime-plugins>` の設定にも利用しています。
..  [#] http://hadoop.apache.org/docs/r1.2.1/api/org/apache/hadoop/conf/Configuration.html

データソースの追加
------------------
データソースを追加するには設定ファイルに次の項目を追加します。

..  list-table:: データソースを追加する際の設定
    :widths: 30 30
    :header-rows: 1

    * - 名前
      - 値
    * - ``com.asakusafw.directio.<DSID>``
      - データソースの実装クラス名
    * - ``com.asakusafw.directio.<DSID>.path``
      - データソースを配置する論理パス

設定の名前に含まれる ``<DSID>`` はそれぞれのデータソースを表す識別子です。
``<DSID>`` には半角アルファベットの大文字小文字、半角数字、半角アンダースコア ( ``_`` ) の組み合わせを指定できます。
複数のデータソースを利用する場合にはデータソースごとに識別子を変えて指定してください。

データソースの実装は、現在のところ `Hadoopのファイルシステムを利用したデータソース`_ のみを提供しています。
詳しくは対象の項を参照してください。

論理パスとはDirect I/Oのそれぞれのデータソースを配置する仮想的なパスで、DSLからこのパスを指定してデータソースを利用します。
このパスは ``alpha/beta/gamma`` のように名前をスラッシュ ( ``/`` ) で区切って書きます。

特別なパスとして、ルートパスは ``/`` 一文字で指定します。


論理パスの解決
--------------
DSLで指定した論理パスから実行時にデータソースを引き当てる際、次のような方法でデータソースの検索が行われます。

#. 論理パスに対してデータソースが配置されている場合、そのデータソースを利用する
#. 論理パスに対してデータソースが配置されていない場合、現在の論理パスの親パスに対して再帰的にデータソースの検索を行う
#. ただし、現在の論理パスがルートである場合、データソースの検索は失敗する

つまり、DSLで指定した論理パスに対して、親方向に最も近いデータソースを検索して利用しています。

また、データソースを配置した論理パスよりもDSLで指定した論理パスの方が長い (つまり、サブパスが指定された) 場合、
データソースを配置した論理パスからの相対パスをファイルパスの先頭に利用します。

たとえば、データソースを ``a/b`` に配置し、DSLでは論理パスに ``a/b/c/d`` と指定した場合、データソースからの相対パスは ``c/d`` となります。さらにDSLでファイルパスに ``e/f`` と指定すると、結果のファイルパスは ``c/d/e/f`` となります。

..  note::
    この論理パスの機構は、Unixのファイルシステムのマウントを参考に設計しています。


Hadoopのファイルシステムを利用したデータソース
----------------------------------------------
データソースの実装として、HadoopのファイルシステムAPI ( ``FileSystem`` [#]_ ) を利用したものを提供しています。

本データソースを利用する場合、実装クラス名 ( ``com.asakusafw.directio.<DSID>`` ) には :javadoc:`com.asakusafw.runtime.directio.hadoop.HadoopDataSource` を指定します。
また、利用するファイルシステムについては、Hadoopの本体側であらかじめ設定を行っておく必要があります。

Direct I/Oの設定ファイルには、対象のデータソースに対してさらに論理パスに対するファイルシステム上のパスを表す「ファイルシステムパス」の設定が必要です。

..  list-table:: Hadoopのファイルシステムを利用したデータソース
    :widths: 30 5 25
    :header-rows: 1

    * - 名前
      - 形式
      - 値
    * - ``com.asakusafw.directio.<DSID>.fs.path``
      - URI
      - ファイルシステム上のパス

..  [#] ``org.apache.hadoop.fs.FileSystem``

.. _directio-filesystem-path-format:

ファイルシステムパスの形式
~~~~~~~~~~~~~~~~~~~~~~~~~~
ファイルシステムパスには次の3種類の形式を指定できます。

``相対パス``
    Hadoopのデフォルトファイルシステム [#]_ のワーキングディレクトリ [#]_ からの相対パスを利用します。
    
    なお、デフォルトファイルシステムにローカルファイルシステムを指定している場合、
    ワーキングディレクトリは必ずユーザーのホームディレクトリになります。

``絶対パス``
    Hadoopのデフォルトファイルシステム上の絶対パスを利用します。
    
    たとえば ``/var/log`` や ``/tmp/directio`` などです。

``完全URI``
    URIに対応するファイルシステム、ホスト、パスを利用します。
    
    たとえば ``file:///home/asakusa`` や ``hdfs://localhost:8020/user/asakusa`` などです。


..  [#] Hadoopの設定ファイル ``core-site.xml`` 内の ``fs.default.name`` に指定したファイルシステムです。
..  [#] 多くのHadoopディストリビューションでは、デフォルトのワーキングディレクトリはアプリケーション実行ユーザのホームディレクトリです。

..  hint::
    ファイルシステムパスの形式は環境や構成に応じて使い分けるべきです。

    例えば開発環境ではOSやHadoopの設定に依存しない相対パスの設定が便利でしょう。運用環境ではワーキングディレクトリに依存しない絶対パスの設定が安定するかもしれません。また、複数種類のデータソースを使用し、Hadoopのデフォルトファイルシステム以外のファイルシステムを利用する場合は完全URIを使用する必要がありますが、この場合すべてのファイルシステムパスを完全URIで記述したほうが可読性が向上するかもしれません。
    
    絶対パスや完全URIはHadoop側の設定を変更した場合に、その設定に追従する必要があるかもしれないので注意が必要です。

..  warning::
    ファイルシステムパス以下はテスト実行時に全て削除されます。
    特にスタンドアロンモードのHadoopを利用時に相対パスを指定した場合、
    ホームディレクトリを起点としたパスと解釈されるため注意が必要です。


論理パスとファイルシステムパスの対応付け
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Hadoopのファイルシステムを利用したデータソースでは、指定したファイルシステム上のパス ( ``com.asakusafw.directio.<DSID>.fs.path`` ) を起点に論理パスとファイルを対応付けます。具体的には、次のような手順で対応付けます。

* DSLで指定した論理パスとファイル名から、 `論理パスの解決`_ にある方法で実際のファイルパスを計算する
* 計算したファイルパスを、指定したファイルシステム上のパスからの相対パスとみなす

たとえば、データソースを ``hadoop`` に配置し、DSLでは論理パスに ``hadoop/asakusa`` , ファイル名に ``data.csv`` と指定した場合、実際に利用するファイルパスは ``asakusa/data.csv`` となります。さらに起点となるファイルシステム上のパスが ``hdfs://localhost/user`` であった場合、対応付けられる最終的なファイルシステム上のパスは ``hdfs://localhost/user/asakusa/data.csv`` となります。


ファイルの分割読み出しの設定
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
`Hadoopのファイルシステムを利用したデータソース`_ において、 `入力データの分割`_ は次のように設定します。
いずれのプロパティも必須ではありません。

..  list-table:: ファイルの分割読み出しに関する設定
    :widths: 30 5 20
    :header-rows: 1
    
    * - 名前
      - 形式
      - 値
    * - ``com.asakusafw.directio.<DSID>.fragment.min``
      - long
      - 断片の最小バイト数
    * - ``com.asakusafw.directio.<DSID>.fragment.pref``
      - long
      - 断片の推奨バイト数

``...fragment.min`` に0未満の値を指定した場合、入力データの分割は行われません。
未指定の場合は 16MB 程度に設定されます。

``...fragment.pref`` が未指定の場合、 64MB程度に設定されます。
また、 ``...fragment.min`` 未満の値は設定できません。

分割の最小バイト数や推奨バイト数はデータの形式で上書きされることがあります。

* データの形式が入力データの分割を許可しない場合、ファイルは分割されない
* データの形式で指定した最小バイト数がデータソースで指定したものより大きな場合、データの形式で指定したものを優先する
* データの形式で推奨バイト数が指定されている場合、データの形式で指定したものを優先する
* 推奨バイト数が最小バイト数未満になる場合、推奨バイト数は最小バイト数の値を利用する

入力データの分割を許可している場合、このデータソースにおいてそれぞれの断片は次の制約をすべて満たします。

* それぞれの断片は最小バイト数未満にならない
* それぞれの断片は推奨バイト数の2倍以上にならない

..  note::
    Hadoop本体に指定したスプリットの設定はここでは使用しません。
    通常の場合は既定の設定値で問題なく動作するはずですが、
    ファイルの途中からデータを読み出すような操作に多大なコストがかかるようなファイルシステムにおいては、
    ファイルの分割を行わないなどの設定が必要になります。

トランザクションの設定
~~~~~~~~~~~~~~~~~~~~~~
`Hadoopのファイルシステムを利用したデータソース`_ において、 `簡易的な出力のトランザクション`_ は次のように設定します。
いずれのプロパティも必須ではありません。

..  list-table:: トランザクションに関する設定
    :widths: 25 5 30
    :header-rows: 1
    
    * - 名前
      - 形式
      - 値
    * - ``com.asakusafw.directio.<DSID>.fs.tempdir``
      - URI
      - トランザクション用に利用するファイルシステム上のパス
    * - ``com.asakusafw.directio.<DSID>.output.staging``
      - boolean
      - ステージング領域を利用するかどうか
    * - ``com.asakusafw.directio.<DSID>.output.streaming``
      - boolean
      - 試行領域に直接出力するかどうか

``...fs.tempdir`` を省略した場合、このパスは ``com.asakusafw.directio.<DSID>.fs.path`` 下の ``_directio_temp`` というディレクトリになります。
明示的に設定を行う場合、この値は  ``...fs.path`` と同一のファイルシステムでなければなりません [#]_ 。

``...output.staging`` を省略した場合、この値は ``true`` (ステージング領域を利用する) となります。
ステージング領域を利用しない場合、タスク試行の完了時に最終的な出力先へ結果のデータを直接移動します。

``...output.streaming`` を省略した場合、この値は ``true`` (試行領域に直接出力する) となります。
試行領域に直接出力しない場合、ローカルテンポラリ領域にファイルを作成したのち、タスク試行成功時にステージング領域にファイルを移動します。
この時利用するローカルテンポラリ領域は `ローカルテンポラリ領域の設定`_ があらかじめ必要です [#]_ 。


トランザクションが修復不可能な状態になった場合や、タスク試行中にHadoopそのものが異常終了した場合、 ``...fs.tempdir`` 以下に処理の途中結果が残されている場合があります。

..  [#] 具体的には、 ``...fs.tempdir`` 以下のファイルを ``...fs.path`` 以下のディレクトリに ``FileSystem.rename()`` で移動できる必要があります。
..  [#] 試行領域に直接出力をしない場合にローカルテンポラリ領域が設定されていないと実行時にエラーとなります。


Keep Aliveの設定
~~~~~~~~~~~~~~~~
Hadoopの一部のファイルシステムでは、データを大きなブロックで転送するような実装になっています。
大きなブロックを転送する際にハートビート信号を送らず、そのような状態が長く続くとタスクがジョブトラッカーによって強制終了されてしまいます。

`Hadoopのファイルシステムを利用したデータソース`_ において、Keep Aliveの設定を行うことで上記の問題を回避できます。
これは、Direct I/Oでデータの転送を行っている裏側で、自動的に疑似的にハートビート信号を送り続けます。

..  list-table:: Keep Aliveの設定
    :widths: 30 5 20
    :header-rows: 1
    
    * - 名前
      - 形式
      - 値
    * - ``com.asakusafw.directio.<DSID>.keepalive.interval``
      - long
      - ハートビート信号を送る間隔 (ミリ秒)

``...keepalive.interval`` を省略した場合、Direct I/OでのKeep Aliveの設定は無効になります。
設定する場合には、タスク試行のタイムアウト時間 [#]_ の半分程度を指定するのが良いでしょう。

..  [#] 通常は 600,000 ミリ秒程度です

..  caution::
    Keep Aliveの設定は注意深く行ってください。
    タスク試行内の処理がフリーズしてしまった場合、通常そのタスク試行はタイムアウトすることになります。
    しかし、上記のKeep Aliveが有効になっていると、まだ動き続けていると見なされてタイムアウトしない場合があります。

..  hint::
    上記の他に、Hadoop本体の設定 ``mapred.task.timeout`` を変更することでも対応可能です。

データソースの設定例
--------------------
ここではいくつかのデータソースの設定例を示します。

HDFSでの設定例
~~~~~~~~~~~~~~
以下はHDFSの入出力を行う場合の設定例です。

..  code-block:: xml

    <property>
        <name>com.asakusafw.directio.hdfs</name>
        <value>com.asakusafw.runtime.directio.hadoop.HadoopDataSource</value>
    </property>
    <property>
        <name>com.asakusafw.directio.hdfs.path</name>
        <value>hdfs/var</value>
    </property>
    <property>
        <name>com.asakusafw.directio.hdfs.fs.path</name>
        <value>hdfs://localhost:8020/var/asakusa</value>
    </property>

HDFSは直接の出力やファイルの移動を低コストで行えるようになっています。
そのため、特別な設定を行わなくてもそれなりに動作します。

Amazon S3での設定例
~~~~~~~~~~~~~~~~~~~
Amazon Simple Storage Service ( `Amazon S3`_ )の入出力を行う場合の設定例です。

..  code-block:: xml

    <property>
        <name>com.asakusafw.directio.s3</name>
        <value>com.asakusafw.runtime.directio.hadoop.HadoopDataSource</value>
    </property>
    <property>
        <name>com.asakusafw.directio.s3.path</name>
        <value>s3/spool</value>
    </property>
    <property>
        <name>com.asakusafw.directio.s3.fs.path</name>
        <value>s3://example/var/spool</value>
    </property>
    <property>
        <name>com.asakusafw.directio.s3.output.staging</name>
        <value>false</value>
    </property>
    <property>
        <name>com.asakusafw.directio.s3.output.streaming</name>
        <value>false</value>
    </property>
    <property>
        <name>com.asakusafw.output.system.dir</name>
        <value>s3://example/var/system</value>
    </property>
    <property>
        <name>com.asakusafw.output.local.tempdir</name>
        <value>/mnt/asakusa-directio</value>
    </property>

2013年11月現在、Hadoopのファイルシステムを経由してS3を利用する場合、出力ファイルの移動にコストがかかるようです。
このため、上記の設定では主に次のようなことを行っています。

* 試行領域をローカルファイルシステム上に作成する ( ``...output.streaming = false`` )

  * 試行領域をS3上に作成した場合、試行の成功の際にファイルの名前を変更します。しかし、S3上でファイルの名前を変更すると、コピーと削除の組み合わせが内部的に発生します。
  * このため、出力を直接データソースに出力せず、ローカルテンポラリ領域に出力するよう設定しています ( ``com.asakusafw.output.local.tempdir`` ) 。

* ステージ領域をスキップする ( ``...output.staging = false`` )

  * ステージ領域を利用する場合、タスクが全て成功した後にファイルの名前変更を行います。試行領域の時と同様に、S3上でのファイル名変更はHDFS上のそれより時間がかかります。

..  attention::
    上記の例はステージ領域をスキップするよう設定していますが、この設定によりトランザクション処理が行えなくなる点に注意してください。

..  attention::
    HadoopやAmazon EMRの古いバージョンなどの一部の実装では、Hadoopのファイルシステムを経由してS3を利用する場合に入力データの分割にコストがかかる（巨大な入力データを途中から読み出す際にウェイトが発生する）ようです。
    
    このような環境では、入力データの分割を行わない ( ``...fragment.min = -1`` ) ように設定することを推奨します。以下設定例です。

    ..  code-block:: xml
    
        <property>
            <name>com.asakusafw.directio.s3.fragment.min</name>
            <value>-1</value>
        </property>


複数のデータソースを利用する設定例
----------------------------------


..  code-block:: xml

    <property>
        <name>com.asakusafw.directio.data</name>
        <value>com.asakusafw.runtime.directio.hadoop.HadoopDataSource</value>
    </property>
    <property>
        <name>com.asakusafw.directio.data.path</name>
        <value>data</value>
    </property>
    <property>
        <name>com.asakusafw.directio.data.fs.path</name>
        <value>hdfs://localhost:8020/user/directio/var</value>
    </property>
    <property>
        <name>com.asakusafw.directio.master</name>
        <value>com.asakusafw.runtime.directio.hadoop.HadoopDataSource</value>
    </property>
    <property>
        <name>com.asakusafw.directio.master.path</name>
        <value>data/master</value>
    </property>
    <property>
        <name>com.asakusafw.directio.master.fs.path</name>
        <value>hdfs://localhost:8020/user/directio/master</value>
    </property>

複数のデータソースを組み合わせて利用する場合、設定ファイルのデータソース( ``com.asakusafw.directio.<DSID>`` ) のうち、 ``<DSID>`` の部分を別々のものに設定します。

上記の例は論理パス ``data`` と ``data/master`` に対してそれぞれ ``data`` , ``master`` というDSIDのデータソースを指定する例です。
論理パスとファイルシステムパスをそれぞれ次のように対応づけています。


..  list-table:: 論理パスとファイルシステムパスの対応付け
    :widths: 5 10 40
    :header-rows: 1

    * - ID
      - 論理パス
      - ファイルシステムパス

    * - ``data``
      - ``data``
      - ``hdfs://localhost:8020/user/directio/var``

    * - ``master``
      - ``data/master``
      - ``hdfs://localhost:8020/user/directio/master``

上記の設定では、DSLから ``data`` というパスが指定された場合に ``data`` というデータソースを利用し、 ``data/master`` というパスが指定された場合に ``master`` というデータソースを利用します。

それ以外に、 ``data/transaction`` や ``data/2012`` など、 ``data`` 以下でなおかつ ``data/master`` と無関係なパスが指定された場合にも ``data`` というデータソースを利用します。
``master`` というデータソースも同様に、 ``data/master/item`` など、 ``data/master`` のサブパスを指定した場合にも利用されます。

DSLで論理パスより長いパスを指定した場合、論理パスにマッチした残りの部分はそのままファイルシステム上のパスに利用します。
上記の設定でDSLから ``data/2012/01`` と指定した場合、実行時には ``hdfs://localhost:8020/user/directio/var/2012/01`` というパスとして処理が行われます。

なお、 ``data`` とは関係ないパス（たとえば ``var/log`` など）が指定された場合には、対応するデータソースが見つからないためエラーとなります。
これを避けるにはデフォルト設定のように、ルートパス ( ``/`` ) に対してデータソースを配置します。

..  hint::
    データソースの識別子(DSID)は実行時のログメッセージにも利用されるため、わかりやすいものにしてください。


その他の設定
============
データソースの設定以外に、Direct I/Oの全体を通した設定を行えます。

システムディレクトリの設定
--------------------------
システムディレクトリはDirect I/Oの管理情報を保持するためのディレクトリで、以下の形式で設定します。
この内容はHadoop本体の設定ファイルに書いても、Direct I/Oの設定ファイルに書いてもどちらでも有効です [#]_ 。

..  list-table:: システムディレクトリの設定
    :widths: 20 5 30
    :header-rows: 1
    
    * - 名前
      - 形式
      - 値
    * - ``com.asakusafw.output.system.dir``
      - URI
      - Hadoopファイルシステム上のシステムディレクトリ

システムディレクトリの設定が省略された場合、Hadoopが利用するデフォルトファイルシステム上の、 ``<ワーキングディレクトリ>/_directio`` を利用します。
またプロパティの値の中に、Javaのシステムプロパティを ``${システムプロパティ名}`` という形式で利用できます。

..  note::
    システムディレクトリはトランザクションの管理情報など、Direct I/Oを利用するうえで重要な情報が記録されます。
    そのため、信頼性の高いデータストア上か、Direct I/Oを利用するうえで重要性の高いデータストアと同じ領域内に配置することを推奨します。

..  [#] 正確に言えば、データソースの設定もHadoop本体の設定ファイル内に記載できます。
    ただし、データソースの設定はDirect I/O独自の設定ファイルに記載することを推奨します。

ローカルテンポラリ領域の設定
----------------------------
ローカルテンポラリ領域は、Direct I/Oが利用するHadoopスレーブノードのローカルファイルシステム上のディレクトリです。
タスク試行の実行中に一時的に利用します [#]_ 。

この内容は以下の形式で設定します。
なお、Hadoop本体の設定ファイルに書いても、Direct I/Oの設定ファイルに書いてもどちらでも有効です。

..  list-table:: ローカルテンポラリ領域の設定
    :widths: 20 10 30
    :header-rows: 1
    
    * - 名前
      - 形式
      - 値
    * - ``com.asakusafw.output.local.tempdir``
      - ファイルパス
      - ローカルファイルシステム上のテンポラリディレクトリ

ローカルテンポラリ領域はローカルファイルシステム上の絶対パスを指定します。
この設定が省略された場合、ローカルテンポラリ領域は利用できなくなります。

設定に対するディレクトリが存在しない場合、ローカルテンポラリ領域の利用時に自動的にディレクトリを作成します。

..  [#] 詳しくは `トランザクションの設定`_ を参照してください。

ログの設定
----------
Direct I/Oに関するログはHadoop本体のログの設定を利用して行います。
Hadoop本体の関連するドキュメントを参照してください。


ファイルの入出力
================
Direct I/Oを利用してファイルを入出力するには、 `Hadoopのファイルシステムを利用したデータソース`_ などの設定をしておきます。

また、データモデルと対象のファイル形式をマッピングする ``DataFormat`` [#]_ の作成が必要です。
``DataFormat`` のサブタイプとして、任意のストリームを取り扱う ``BinaryStreamFormat`` [#]_ や、Hadoopのファイルを取り扱う ``HadoopFileFormat`` [#]_ を現在利用できます ( ``DataFormat`` は直接実装できません ) 。

上記のうち、CSVファイルを読み書きするための実装クラスは、DMDLコンパイラの拡張 [#]_ を利用して自動的に生成できます。

なお、以降の機能を利用するには次のライブラリやプラグインが必要です [#]_ 。

..  list-table:: Direct I/Oで利用するライブラリ等
    :widths: 50 50
    :header-rows: 1

    * - ライブラリ
      - 概要
    * - ``asakusa-directio-vocabulary``
      - DSL用のクラス群
    * - ``asakusa-directio-plugin``
      - DSLコンパイラプラグイン
    * - ``asakusa-directio-test-moderator``
      - テストドライバプラグイン
    * - ``asakusa-directio-dmdl``
      - DMDLコンパイラプラグイン


..  [#] :javadoc:`com.asakusafw.runtime.directio.DataFormat`
..  [#] :javadoc:`com.asakusafw.runtime.directio.BinaryStreamFormat`
..  [#] :javadoc:`com.asakusafw.runtime.directio.hadoop.HadoopFileFormat`
..  [#] :doc:`../dmdl/user-guide` を参照
..  [#] バージョン |version| ではすべてのアーキタイプでこれらのライブラリやプラグインがSDKアーティファクトという依存性定義によってデフォルトで設定されています。詳しくは :doc:`../application/maven-archetype` や :doc:`../application/sdk-artifact` を参照してください。

CSV形式のDataFormatの作成
-------------------------
CSV形式 [#]_ に対応した ``DataFormat`` の実装クラスを自動的に生成するには、対象のデータモデルに対応するDMDLスクリプトに ``@directio.csv`` を指定します。

..  code-block:: none

    @directio.csv
    document = {
        "the name of this document"
        name : TEXT;

        "the content of this document"
        content : TEXT;
    };

上記のように記述してデータモデルクラスを生成すると、 ``<出力先パッケージ>.csv.<データモデル名>CsvFormat`` というクラスが自動生成されます。
このクラスは ``DataFormat`` を実装し、データモデル内のプロパティが順番に並んでいるCSVを取り扱えます。

また、 単純な `ファイルを入力に利用するDSL`_ と `ファイルを出力に利用するDSL`_ の骨格も自動生成します。前者は ``<出力先パッケージ>.csv.Abstract<データモデル名>CsvInputDescription`` 、後者は ``<出力先パッケージ>.csv.Abstract<データモデル名>CsvOutputDescription`` というクラス名で生成します。必要に応じて継承して利用してください。

この機能を利用するには、DMDLコンパイラのプラグインに ``asakusa-directio-dmdl`` を追加する必要があります。
DMDLコンパイラについては :doc:`../dmdl/user-guide` を参照してください。

..  note::
    この機構は :doc:`WindGate <../windgate/user-guide>` のものと将来統合されるかもしれません。

..  [#] ここでのCSV形式は、RFC 4180 (http://www.ietf.org/rfc/rfc4180.txt) で提唱されている形式を一部変更したものです。
    文字セットをASCIIの範囲外にも拡張したり、CRLF以外にもLFのみも改行と見なしたり、ダブルクウォート文字の取り扱いを緩くしたりなどの拡張を加えています。
    `CSV形式の注意点`_ も参照してください。


CSV形式の設定
~~~~~~~~~~~~~
``@directio.csv`` 属性には、次のような要素を指定できます。

..  list-table:: CSV形式の設定
    :widths: 10 10 20 60
    :header-rows: 1

    * - 要素
      - 型
      - 既定値
      - 内容
    * - ``charset``
      - 文字列
      - ``"UTF-8"``
      - ファイルの文字エンコーディング
    * - ``allow_linefeed``
      - 論理値
      - ``FALSE``
      - ``TRUE`` で値内にLFを含められる。 ``FALSE`` で不許可
    * - ``has_header``
      - 論理値
      - ``FALSE``
      - ``TRUE`` でヘッダの利用を許可。 ``FALSE`` で不許可
    * - ``true``
      - 文字列
      - ``"true"``
      - ``BOOLEAN`` 型の ``TRUE`` 値の表現形式
    * - ``false``
      - 文字列
      - ``"false"``
      - ``BOOLEAN`` 型の ``FALSE`` 値の表現形式
    * - ``date``
      - 文字列
      - ``"yyyy-MM-dd"``
      - ``DATE`` 型の表現形式
    * - ``datetime``
      - 文字列
      - ``"yyyy-MM-dd HH:mm:ss"``
      - ``DATETIME`` 型の表現形式
    * - ``compression``
      - 文字列
      - なし
      - ファイルの圧縮形式

なお、 ``date`` および ``datetime`` には ``SimpleDateFormat`` [#]_ の形式で日付や時刻を指定します。

また、 ``compression`` には、 ``"gzip"`` または ``CompressionCodec`` [#]_ のサブタイプのクラス名を指定します [#]_ 。
ここで指定した圧縮形式で対象のファイルが読み書きされるようになりますが、代わりに `入力データの分割`_ が行われなくなります。

..  attention::
    デフォルトでは ``allow_linefeed`` には ``FALSE`` が設定されていて、文字列の内部などに改行文字 LF を含められないようになっています。
    この設定を ``TRUE`` にすることでLFを含められるようになりますが、代わりに `入力データの分割`_ が行われなくなります。
    詳しくは `CSV形式の注意点`_ を参照してください。

以下はDMDLスクリプトの記述例です。

..  code-block:: none

    @directio.csv(
        charset = "ISO-2022-JP",
        allow_linefeed = TRUE,
        has_header = TRUE,
        true = "1",
        false = "0",
        date = "yyyy/MM/dd",
        datetime = "yyyy/MM/dd HH:mm:ss",
        compression = "gzip",
    )
    model = {
        ...
    };

..  [#] ``java.text.SimpleDateFormat``
..  [#] ``org.apache.hadoop.io.compress.CompressionCodec``
..  [#] ``org.apache.hadoop.io.compress.DefaultCodec`` などが標準で用意されています

ヘッダの設定
~~~~~~~~~~~~
`CSV形式の設定`_ でヘッダを有効にしている場合、出力の一行目にプロパティ名が表示されます。
ここで表示される内容を変更するには、それぞれのプロパティに ``@directio.csv.field`` 属性を指定し、さらに ``name`` 要素でフィールド名を指定します。

以下はヘッダの内容の付加したDMDLスクリプトの記述例です。

..  code-block:: none

    @directio.csv
    document = {
        "the name of this document"
        @directio.csv.field(name = "題名")
        name : TEXT;

        "the content of this document"
        @directio.csv.field(name = "内容")
        content : TEXT;
    };

ファイル情報の取得
~~~~~~~~~~~~~~~~~~
解析中のCSVファイルに関する属性を取得する場合、それぞれ以下の属性をプロパティに指定します。

..  list-table:: ファイル情報の取得に関する属性
    :widths: 4 2 4
    :header-rows: 1

    * - 属性
      - 型
      - 内容
    * - ``@directio.csv.file_name``
      - ``TEXT``
      - ファイル名
    * - ``@directio.csv.line_number``
      - ``INT`` , ``LONG``
      - テキスト行番号 (1起算)
    * - ``@directio.csv.record_number``
      - ``INT`` , ``LONG``
      - レコード番号 (1起算)

上記の属性が指定されたプロパティは、CSVのフィールドから除外されます。

..  attention::
    ``@directio.csv.line_number`` または ``@directio.csv.record_number`` が指定された場合、 `入力データの分割`_ が行われなくなります。
    詳しくは `CSV形式の注意点`_ を参照してください。

..  attention::
    これらの属性はCSVの解析時のみ有効です。
    CSVを書き出す際には無視されます。

CSVから除外するプロパティ
~~~~~~~~~~~~~~~~~~~~~~~~~
特定のプロパティをCSVのフィールドとして取り扱いたくない場合、プロパティに ``@directio.csv.ignore`` を指定します。

CSV形式の注意点
~~~~~~~~~~~~~~~
自動生成でサポートするCSV形式を利用するうえで、いくつかの注意点があります。

* 改行文字は CRLF または LF のみ、CRのみです

  * ただしCRのみを利用している場合、入力データの分割が正しく行われません

* CSVに空の文字列を書き出しても、読み出し時に ``null`` として取り扱われます
* 論理値は復元時に、値が ``true`` で指定した文字列の場合には ``true`` , 空の場合には ``null`` , それ以外の場合には ``false`` となります
* ヘッダが一文字でも異なる場合、解析時にヘッダとして取り扱われません
* 1レコードが10MBを超える場合、正しく解析できません
* 以下のいずれかが指定された場合、 `入力データの分割`_ は行われなくなります

  * ``@directio.csv( compression = ... )``
  * ``@directio.csv( allow_linefeed = TRUE )``
  * ``@directio.csv.line_number``
  * ``@directio.csv.record_number``


シーケンスファイル形式のDataFormatの作成
----------------------------------------
Hadoopのシーケンスファイル [#]_ を直接読み書きするには、 ``SequenceFileFormat`` [#]_ のサブクラスを作成します。

..  hint::
    以降の記述は、Asakusa Frameworkの外部で作成されたシーケンスファイルを利用する際の方法です。
    シーケンスファイルにAsakusa Frameworkのデータモデル形式を直接利用する場合 `内部データ形式を利用したシーケンスファイル形式のDataFormatの作成`_ なども利用可能です。

``SequenceFileFormat`` は ``HadoopFileFormat`` のサブクラスで、シーケンスファイルを読み書きするための骨格実装が提供されています。

このクラスを継承する際には、以下の型引数を ``SequenceFileFormat<K, V, T>`` にそれぞれ指定してください。

``K``
    対象シーケンスファイルのキーオブジェクトの型

``V``
    対象シーケンスファイルの値オブジェクトの型

``T``
    アプリケーションで利用するデータモデルオブジェクトの型

このクラスでは、下記のメソッドをオーバーライドします。

``Class<T> getSupportedType()``
    対象となるデータモデルのクラスを戻り値に指定します。

``K createKeyObject()``
    対象のシーケンスファイルのキーと同じクラスのオブジェクトを戻り値に指定します。

``V createValueObject()``
    対象のシーケンスファイルの値と同じクラスのオブジェクトを戻り値に指定します。

``void copyToModel(K key, V value, T model)``
    シーケンスファイルから読み出したキー ( ``key`` ) と 値 ( ``value`` ) の内容を、対象のデータモデルオブジェクト ( ``model`` ) に設定します。
    このメソッドは、シーケンスファイルからデータ読み出す際に、レコードごとに起動されます。
    このメソッドによって変更されたデータモデルオブジェクトは、以降の処理の入力として利用されます。

``void copyFromModel(T model, K key, V value)``
    結果を表すデータモデルオブジェクトの内容を、シーケンスファイルのキー ( ``key`` ) と値 ( ``value`` ) に設定します。
    このメソッドは、シーケンスファイルにデータを書き込む際に、レコードごとに起動されます。
    このメソッドによって変更されたキーと値がそのままシーケンスファイルに書き出されます。

``CompressionCodec getCompressionCodec(Path path)``
    シーケンスファイルの作成時に利用する圧縮コーデックを指定します。

    オーバーライドしない場合、全体の設定情報をもとに圧縮コーデックを決定します。
    詳しくは `シーケンスファイルの圧縮`_ を参照してください。

以下はシーケンスファイル形式のDataFormatの実装例です。

..  code-block:: java

    public class ExampleSequenceFormat extends SequenceFileFormat<LongWritable, Text, MyData> {

        @Override
        public Class<MyData> getSupportedType() {
            return MyData.class;
        }

        @Override
        protected LongWritable createKeyObject() {
            return new LongWritable();
        }

        @Override
        protected Text createValueObject() {
            return new Text();
        }

        @Override
        protected void copyToModel(LongWritable key, Text value, MyData model) {
            model.setPosition(key.get());
            model.setText(value);
        }

        @Override
        protected void copyFromModel(MyData model, LongWritable key, Text value) {
            key.set(model.getPositionOption().or(0L));
            value.set(model.getTextOption().or("(null)"));
        }
    }

..  hint::
    この機能は、 `Apache Sqoop`_ 等のツールと連携することを想定して提供されています。

..  [#] ``org.apache.hadoop.io.SequenceFile``
..  [#] :javadoc:`com.asakusafw.runtime.directio.hadoop.SequenceFileFormat`

..  _`Apache Sqoop` : http://sqoop.apache.org/

シーケンスファイルの圧縮
~~~~~~~~~~~~~~~~~~~~~~~~
``SequenceFileFormat`` を利用してシーケンスファイルを作成する場合、以下のいくつかの方法で圧縮形式を指定できます。
以下、上から順に該当する項目があれば、そこで設定された圧縮形式を利用します。

``SequenceFileFormat.getCompressionCodec(Path path)`` をオーバーライド
    オーバーライドしたメソッドが返す圧縮コーデックを利用します。

    ``null`` を指定した場合、圧縮は行われません。

設定ファイルで ``com.asakusafw.output.sequencefile.compression.codec`` を指定
    上記の設定値に ``CompressionCodec`` [#]_ を実装したクラス名を指定すると、その圧縮コーデックを利用します。

    なお、利用する圧縮コーデックはあらかじめHadoopクラスターの全台に導入されている必要があります。

上記いずれの指定もない場合、シーケンスファイルの圧縮を行いません。

..  note::
    上記の設定はシーケンスファイル作成時のみ有効です。
    シーケンスファイルを読み出す際には、シーケンスファイルの圧縮形式を自動的に判別します。

..  [#] ``org.apache.hadoop.io.compress.CompressionCodec``

内部データ形式を利用したシーケンスファイル形式のDataFormatの作成
----------------------------------------------------------------
シーケンスファイル対し、Asakusa Frameworkで利用するデータモデル形式を直接保存したり復元したりするような ``DataFormat`` の実装クラスを自動的に生成するには、対象のデータモデルに ``@directio.sequence_file`` を指定します。

..  code-block:: none

    @directio.sequence_file
    document = {
        "the name of this document"
        name : TEXT;

        "the content of this document"
        content : TEXT;
    };

上記のように記述してデータモデルクラスを生成すると、 ``<出力先パッケージ>.sequencefile.<データモデル名>SequenceFileFormat`` というクラスが自動生成されます。
このクラスは ``DataFormat`` を実装し、対象のデータモデルオブジェクトをHadoopの直列化機構を直接利用したシーケンスファイルを取り扱えます。

また、 単純な `ファイルを入力に利用するDSL`_ と `ファイルを出力に利用するDSL`_ の骨格も自動生成します。前者は ``<出力先パッケージ>.sequencefile.Abstract<データモデル名>SequenceFileInputDescription`` 、後者は ``<出力先パッケージ>.sequencefile.Abstract<データモデル名>SequenceFileOutputDescription`` というクラス名で生成します。必要に応じて継承して利用してください。

この機能を利用するには、DMDLコンパイラのプラグインに ``asakusa-directio-dmdl`` を追加する必要があります。
DMDLコンパイラについては :doc:`../dmdl/user-guide` を参照してください。

..  warning::
    シーケンスファイルの形式や、内部データのバイナリ表現はHadoopやAsakusa Frameworkのメジャーバージョンアップの際に変更になる場合があります。
    データを長期にわたって保管する場合、CSVなどのポータブルな形式を利用することを推奨します。

..  hint::
    DMDLのデータモデル定義で、同一のデータモデルに ``@directio.csv`` と ``@directio.sequence_file`` の両方を指定することもできます。

..  note::
    シーケンスファイルの中身をテキスト形式で確認する場合、以下のコマンドを利用すると便利です。

    ..  code-block:: sh
    
        hadoop fs -libjars "$ASAKUSA_HOME/core/lib/asakusa-runtime-all.jar,$ASAKUSA_HOME/batchapps/<バッチID>/lib/jobflow-<フローID>.jar" -text "<path/to/sequence-file>"


ファイルを入力に利用するDSL
---------------------------
Direct I/Oを利用してファイルからデータを読み出す場合、 ``DirectFileInputDescription`` [#]_ クラスのサブクラスを作成して必要な情報を記述します。

このクラスでは、下記のメソッドをオーバーライドします。

``String getBasePath()``
    入力に利用する論理パスを戻り値に指定します。

    ここには ``${変数名}`` の形式で、バッチ起動時の引数やあらかじめ宣言された変数を利用できます。
    利用可能な変数はコンテキストAPIで参照できるものと同様です。

``String getResourcePattern()``
    入力に利用するファイル名のパターンを戻り値に指定します。
    ``getBasePath()`` で指定したパスを起点に、このパターンの名前を持つファイルを検索します。

    形式については `入力ファイル名のパターン`_ を参照してください。

``Class<?> getModelType()``
    処理対象とするモデルオブジェクトの型を表すクラスを戻り値に指定します。

    このメソッドは、自動生成される骨格ではすでに宣言されています。

``Class<? extends DataFormat<?>> getFormat()``
    ``DataFormat`` の実装クラスを戻り値に指定します。

    このメソッドは、自動生成される骨格ではすでに宣言されています。

``DataSize getDataSize()``
    入力の推定データサイズを返します。

    省略した場合、データサイズは不明 ( ``DataSize.UNKNOWN`` ) となります。

以下は実装例です。

..  code-block:: java

    public class DocumentFromFile extends DirectFileInputDescription {

        @Override
        public String getBasePath() {
            return "example";
        }

        @Override
        public String getResourcePattern() {
            return "**/data-*.csv";
        }

        @Override
        public Class<?> getModelType() {
            return Document.class;
        }

        @Override
        public Class<? extends DataFormat<?>> getFormat() {
            return DocumentCsvFormat.class;
        }

        @Override
        public DataSize getDataSize() {
            return DataSize.LARGE;
        }
    }

..  [#] :javadoc:`com.asakusafw.vocabulary.directio.DirectFileInputDescription`

入力ファイルのベースパス
~~~~~~~~~~~~~~~~~~~~~~~~
``getBasePath()`` に指定した論理パスは「ベースパス」と呼ばれます。

実行時にはこのベースパスのみを利用して入力元のデータソースを探します。
そのため、以下の2つでは異なる結果になる場合があります。

* ``basePath = "data/asakusa"`` , ``resourcePattern = "file.csv"``
* ``basePath = "data"`` , ``resourcePattern = "asakusa/file.csv"``

上記の場合、 ``data/asakusa`` という論理パスにデータソースが配置されている場合、
それぞれが参照するデータソースは異なるものになります。
この規則について詳しくは、 `論理パスの解決`_ を参照してください。

また、ベースパスには ``${変数名}`` の形式でバッチ引数を利用できます。

入力ファイル名のパターン
~~~~~~~~~~~~~~~~~~~~~~~~
``getResourcePattern()`` にはファイル名だけでなくワイルドカードなどのパターン用の文字列も利用できます。

ここに利用できるパターンは以下の通りです。

..  list-table:: 利用できるパターン
    :widths: 10 10 40
    :header-rows: 1

    * - 文字列
      - 名前
      - 概要
    * - 名前文字
      - リテラル
      - そのままファイル名として利用します。
        対象のデータソースが利用できるファイル名のうち、
        ``/`` , ``\`` , ``$`` , ``*`` , ``?`` , ``#`` , ``|`` , ``{`` , ``}`` , ``[`` , ``]`` 以外の文字を利用できます。
    * - ``/``
      - 名前区切り
      - パスに含まれる名前の区切り文字です。
    * - ``${バッチ引数名}``
      - 変数
      - 実行時にバッチ引数と置き換えます。
        対象のバッチ引数は、変数を含まない任意のパターンの組み合わせである必要があります。
    * - ``*``
      - ワイルドカード
      - 0個以上の任意の名前文字とマッチします。
    * - ``{..|..|..}``
      - 選択
      - ``|`` で区切られたいずれかの名前にマッチします。
        ``..`` の部分には名前文字と名前区切りの組み合わせのみを指定できます。

上記のほかに、特別なディレクトリやファイル名として ``**`` を利用できます。
これは、検索対象以下のすべてのサブディレクトリ(自身のディレクトリも含む)とそれに含まれるファイルにマッチします。

ただし、 ``**`` はディレクトリやファイル名の一部としては利用できません。
たとえば、 ``**.csv`` というパターンは利用できず、代わりに ``**/*.csv`` と書きます。

..  note::
    「変数」に関する挙動は、パターンの解釈の前に一度変数をすべて展開し、
    展開後の文字列をパターンとして解釈して利用しています。

ファイルを出力に利用するDSL
---------------------------
Direct I/Oを利用してファイルからデータを読み出す場合、 ``DirectFileOutputDescription`` [#]_ クラスのサブクラスを作成して必要な情報を記述します。

このクラスでは、下記のメソッドをオーバーライドします。

``String getBasePath()``
    出力に利用する論理パスを戻り値に指定します。

    ここには ``${変数名}`` の形式で、バッチ起動時の引数やあらかじめ宣言された変数を利用できます。
    利用可能な変数はコンテキストAPIで参照できるものと同様です。

``String getResourcePattern()``
    出力に利用するファイル名のパターンを戻り値に指定します。
    ``getBasePath()`` で指定したパスを起点に、このパターンが表すパスにそれぞれのファイルを出力します。

    パターンには ``{property_name:format}`` (プレースホルダ) などを利用できます。
    これは指定したプロパティの内容を、指定のフォーマットでファイル名に埋め込みます。

    詳しくは `出力ファイル名のパターン`_ を参照してください。

``List<String> getOrder()``
    それぞれの出力ファイルの内容をソートするプロパティを指定します。
    
    それぞれのプロパティは ``+property_name`` で昇順、 ``-property_name`` で降順を表します。
    プロパティ名はDMDLのプロパティ名と同様、すべて小文字で単語をアンダースコア ( ``_`` ) で区切ってください。

    省略した場合、出力ファイルのソートを行いません。

``List<String> getDeletePatterns()``
    出力を行う前に削除するファイル名パターンの一覧を戻り値に指定します。
    ``getBasePath()`` で指定したパスを起点に、これらのパターンが表すパスを消去した後に、ファイルの出力を行います。

    パターンには ``*`` (ワイルドカード) など、 `入力ファイル名のパターン`_ と同様のものを利用できます。

    省略した場合、ファイルの削除を行いません。

``Class<?> getModelType()``
    処理対象とするモデルオブジェクトの型を表すクラスを戻り値に指定します。

    このメソッドは、自動生成される骨格ではすでに宣言されています。

``Class<? extends DataFormat<?>> getFormat()``
    ``DataFormat`` の実装クラスを戻り値に指定します。

    このメソッドは、自動生成される骨格ではすでに宣言されています。

以下は実装例です。

..  code-block:: java

    public class DocumentToFile extends DirectFileOutputDescription {

        @Override
        public String getBasePath() {
            return "example";
        }

        @Override
        public String getResourcePattern() {
            return "{date:yyyy/MM}/data.csv";
        }

        @Override
        public List<String> getOrder() {
            return Arrays.asList("+id");
        }

        @Override
        public List<String> getDeletePatterns() {
            return Arrays.asList("${oldyear}/*/data.csv");
        }

        @Override
        public Class<?> getModelType() {
            return Document.class;
        }

        @Override
        public Class<? extends DataFormat<?>> getFormat() {
            return DocumentCsvFormat.class;
        }
    }
.. **

..  note::
    出力先のファイルがすでに存在する場合、古いファイルを削除してからこの出力で上書きします。
    ただし、ファイルに対するレコードがひとつも存在しない場合にはファイル自体が作成されず、古いファイルが残ってしまう場合があります。
    出力先にワイルドカードやランダムな値を利用する場合には、 ``getDeletePatterns()`` を利用してファイルを削除しておいたほうが良い場合があります。

..  [#] :javadoc:`com.asakusafw.vocabulary.directio.DirectFileOutputDescription`

出力ファイルのベースパス
~~~~~~~~~~~~~~~~~~~~~~~~
``getBasePath()`` に指定した論理パスは「ベースパス」と呼ばれます。

実行時にはこのベースパスのみを利用して出力先のデータソースを探します。
そのため、以下の2つでは異なる結果になる場合があります。

* ``basePath = "data/asakusa"`` , ``resourcePattern = "file.csv"``
* ``basePath = "data"`` , ``resourcePattern = "asakusa/file.csv"``

上記の場合、 ``data/asakusa`` という論理パスにデータソースが配置されている場合、
それぞれが参照するデータソースは異なるものになります。
この規則について詳しくは、 `論理パスの解決`_ を参照してください。

また、ベースパスには ``${変数名}`` の形式でバッチ引数を利用できます。

出力ファイルのベースパスは、次のような制約があります。

* 同一ジョブフローの入力が、ある出力のベースパスと同じまたはそのサブパスであってはならない
* 同一ジョブフローの出力が、ある出力のベースパスと同じまたはそのサブパスであってはならない

..  note::
    上記の制約はトランザクションの制御やテストのために導入した制約です。
    出力に対してはこのような制約がありますが、2つの入力が同じベースパスを利用することは可能です。


出力ファイル名のパターン
~~~~~~~~~~~~~~~~~~~~~~~~
``getResourcePattern()`` にはファイル名だけでなくプロパティの内容からファイル名を計算するための、プレースホルダも利用できます。

ここに利用できるパターンは以下の通りです。

..  list-table:: 出力ファイル名に利用できるパターン
    :widths: 2 2 6
    :header-rows: 1

    * - 文字列
      - 名前
      - 概要
    * - 名前文字
      - リテラル
      - そのままファイル名として利用します。
        対象のデータソースが利用できるファイル名のうち、
        ``/`` , ``\`` , ``$`` , ``*`` , ``?`` , ``#`` , ``|`` , ``{`` , ``}`` , ``[`` , ``]`` 以外の文字を利用できます。
    * - ``/``
      - 名前区切り
      - パスに含まれる名前の区切り文字です。
    * - ``${バッチ引数名}``
      - 変数
      - 実行時にバッチ引数と置き換えます。
        対象のバッチ引数は、名前文字または名前区切りの組み合わせである必要があります。
    * - ``{property:format}``
      - プレースホルダ
      - プロパティの内容を指定のフォーマットで文字列化して利用します。
        プロパティはDMDLと同様に ``snake_case`` の形式でプロパティ名を指定します。
    * - ``[開始番号..終了番号]``
      - ランダムな値
      - 開始番号以上、終了番号以下のランダムな数値に置き換えます。
        それぞれの番号は0以上かつ2の31乗未満で、開始番号より終了番号のほうが大きな数値である必要があります。
    * - ``*``
      - ワイルドカード
      - 分散環境上での出力に都合のよい任意の文字列を利用します。
        ただし、この出力にはプレースホルダとランダムな値、およびファイル内のソート機能 [#]_ を利用できなくなります。

..  hint::
    出力ファイルが1つになってしまう場合や、出力ファイルのサイズに大きな偏りができてしまう場合、
    「ランダムな値」を利用することでパフォーマンスを向上させられる場合があります。

..  hint::
    「ランダムな値」をゼロ埋めしたい場合、 ``[0..9][0..9]`` のように書けます。

..  hint::
    「ワイルドカード」は制約が多い代わりに高速に動作する可能性があります。
    MapReduce処理において、他の処理は分散環境上でファイルの単一化の処理が必要になるためReducerを利用しますが、ワイルドカードの場合にはMapperでファイルを直接生成します。
    作成されたファイルを別のDirect I/Oを利用するバッチで読む等であれば、こちらを利用したほうが性能的に有利な場合があります。

..  attention::
    出力するレコード数が「ランダムな値」の範囲よりも十分に大きくない場合、ランダムな値のすべての範囲に対するファイルが生成されない場合があります。

..  warning::
    出力ファイル名のパターンでは、変数の展開後の文字列にプレースホルダ、ランダムな値、ワイルドカードを表す文字列を含められません。
    この制約は将来緩和されるかもしれません。

プレースホルダ ( ``{property:format}`` ) には次のようなフォーマットを利用できます。

..  list-table:: プレースホルダに使用できるフォーマット
    :widths: 20 10 60
    :header-rows: 1

    * - 形式
      - データ型
      - 概要
    * - ``:`` とそれ以降を省略
      - すべて
      - ``toString()`` によって文字列化
    * - ``:<日付>``
      - ``DATE``
      - ``:`` 以降を ``SimpleDateFormat.format()`` によって文字列化
    * - ``:<日時>``
      - ``DATETIME``
      - ``:`` 以降を ``SimpleDateFormat.format()`` によって文字列化

``<日付>`` や ``<日時>`` には ``SimpleDateFormat``  [#]_ 形式のパターンを指定します。
たとえば、パターンに ``data/{date:yyyy/MM}.csv`` と指定すると、プロパティ ``date`` の内容を元に ``data/<年>/<月>.csv`` のようなファイルを年と月の情報からそれぞれ作成します。さらに内容をソートするプロパティにも ``date`` を指定すると、ファイルを年と月で分割した後に日にちでソートして出力できます。

出力ファイル名については `出力ファイルの分割と内容のソート`_ も参照してください。

..  attention::
    出力するデータが存在しない場合、ファイルは一つも作成されません。
    これは、ファイル名にプレースホルダを指定していない場合でも同様です。

..  [#] ``DirectFileOutputDescription.getOrder()`` ( `ファイルを出力に利用するDSL`_ を参照 ) 
..  [#] ``java.text.SimpleDateFormat``


アプリケーションのテスト
========================
Direct I/Oを利用したジョブフローやバッチのテストは、Asakusa Frameworkの通常のテスト方法で行えます。
通常のテストについては :doc:`../testing/index` を参照してください。

なおテスト実行時には、Direct I/Oの設定は開発環境にインストールしたAsakusa Frameworkの設定ファイル ``$ASAKUSA_HOME/core/conf/asakusa-resources.xml`` が使用されるため、必要に応じてこのファイルを編集し、適切な設定を行ってください。

..  attention::
    現在、ジョブフローの出力に対する初期データの作成 ( ``.prepare()`` ) はサポートしていません。

以下はテスト実行時のテストドライバの挙動です。

入出力のクリア
--------------
テストドライバの入出力が指定された場合、テストの実施前に入出力の対象がすべて削除されます。
このとき、DSLの ``getBasePath()`` で指定した論理パス以下のすべての内容を削除します。

..  warning::
    上記のような挙動のため、データソースの入出力対象はできるだけ制限するようにしてください。

入力データの作成
----------------
入力データの作成時、指定された入力ファイルのパターンに対して一つだけファイルを作成します。
この時、下記のルールをもとに作成するファイルパスを計算します。

..  list-table:: テスト時の入力ファイル名の変換ルール
    :widths: 3 2 5
    :header-rows: 1

    * - 文字列
      - 名前
      - 変換後
    * - 名前文字
      - リテラル
      - そのまま利用します
    * - ``/``
      - 名前区切り
      - そのまま利用します
    * - ``${バッチ引数名}``
      - 変数
      - テストに指定したバッチ引数で置き換えます
    * - ``*``
      - ワイルドカード
      - ``__testing__`` という文字列に置き換えます
    * - ``{..|..|..}``
      - 選択
      - 最左の文字列をそのまま利用します

..  note::
    この規則は暫定的なもので、将来変更されるかもしれません。

出力データの取得
----------------
出力された結果データの取得時、テストドライバはDSLの ``getBasePath()`` で指定した論理パス以下のすべての内容を取得します。
このため、バッチのテストで複数のジョブフローが同一のベースパスに出力を行う場合、正しく動作しません。

..  note::
    この規則は暫定的なもので、将来変更されるかもしれません。


トランザクションのメンテナンス
==============================
Direct I/Oのファイル出力時には、 `簡易的な出力のトランザクション`_ を行っています。
出力を開始する前にシステムディレクトリ [#]_ に対してトランザクションの情報を作成し、
トランザクション処理の完了後にこれらの情報をクリアしています。

以降では、トランザクションが中断された際にこれらを手動で修復する方法について紹介します。
なお、いずれのメンテナンス用コマンドについても、コマンドを起動した環境のHadoopのログ設定 [#]_ を利用してログを出力します。

..  [#] 設定方法については `システムディレクトリの設定`_ を参照してください。
..  [#] 設定方法については `ログの設定`_ を参照してください。

トランザクション情報の一覧を表示
--------------------------------
残っているトランザクション情報の一覧を表示するには、 ``$ASAKUSA_HOME/directio/bin/list-transaction.sh`` コマンドを引数なしで実行します。
このコマンドを実行すると、以下の情報を表示します。

..  list-table:: 表示されるトランザクションの情報
    :widths: 4 6
    :header-rows: 1

    * - セクション
      - 内容
    * - ``Date``
      - トランザクションを開始した日時
    * - ``Execution ID``
      - 対象のジョブフローの実行ID
    * - ``Status``
      - トランザクションの状態
    * - ``Comments``
      - 補助的な情報

上記のうち、 ``Status`` を調べることで対象のトランザクションの状態が分かります。
特に重要な状態は ``Committed`` (コミット済み) で、この場合には最終的な出力先が不整合な状態になっている場合があります。

また、以降のコマンドでは ``Execution ID`` (実行ID) の情報を元にトランザクションの修復操作を行います。

コミットの適用
--------------
コミット済みのトランザクションを最終的な出力先に反映させるには、 ``$ASAKUSA_HOME/directio/bin/apply-transaction.sh`` コマンドを実行します。
コマンドの引数にはトランザクションに対応する実行IDを指定してしてください。

このコマンドが対象とするトランザクション処理は、 ``Committed`` (コミット済み) でなければなりません。
それ以外のトランザクション処理に対してこのコマンドを実行しても何も行いません。

このコマンドの実行が成功した場合、トランザクション情報の一覧にコマンドの対象が出現しなくなります。

このコマンドの実行に失敗した場合、出力先のデータソースに何らかの異常が発生している可能性があります。
データソースを正常な状態に戻した後に再度コミットを適用するか、または `トランザクションの破棄`_ を実行して出力に不整合があるままトランザクションを破棄できます。

..  warning::
    コミットを適用する順序には注意が必要です。
    先に適用した出力は、後に適用した出力で上書きされてしまいます。

..  note::
    このコマンドでは、ベストエフォートでのコミットの適用を行っています。
    複数のデータソースが存在し、そのうち一つが常にコミットの適用に失敗してしまう場合、
    即座に適用処理を停止せずにほかのデータソースに対してコミットを適用したのち、エラーとしています。

トランザクションの破棄
----------------------
任意のトランザクション処理を破棄するには、 ``$ASAKUSA_HOME/directio/bin/abort-transaction.sh`` コマンドを実行します。
コマンドの引数にはトランザクションに対応する実行IDを指定してしてください。

..  warning::
    このコマンドはトランザクションのロールバックを行う **のではなく** 、単にトランザクションを破棄します。
    ``Committed`` (コミット済み) のトランザクションに対してこの処理を実行すると、最終的な出力は不整合な状態になる場合があります。

このコマンドの実行が成功した場合、トランザクション情報の一覧にコマンドの対象が出現しなくなります。
また、それぞれのデータソース上でステージング領域や試行領域の中間データを削除します。

..  attention::
    ただし、ローカルテンポラリ領域 [#]_ 内の試行領域については削除されません。
    これらは別の手段で削除する必要があります。

このコマンドの実行に失敗した場合、出力先のデータソースに何らかの異常が発生している可能性があります。
データソースを正常な状態に戻した後に再度実行するか、またはコマンド実行時のログを参考に、トランザクション情報自体を削除してください。

..  [#] `ローカルテンポラリ領域の設定`_ を参照してください。

